{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "from lxml import html\n",
    "import urllib.parse\n",
    "import glob\n",
    "import openai\n",
    "from tqdm.notebook import tqdm\n",
    "from pydantic import BaseModel, field_validator\n",
    "from datetime import datetime\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_urls = [\n",
    "    \"https://meetstamps.com/中国清代邮票目录\",\n",
    "    \"https://meetstamps.com/中华民国邮票目录\",\n",
    "    \"https://meetstamps.com/中国解放区邮票目录\",\n",
    "    \"https://meetstamps.com/中华人民共和国邮票目录\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we are going to get all the urls of the stamps\n",
    "target_urls = []\n",
    "for url in period_urls:\n",
    "    collection_name = urllib.parse.unquote(url.split(\"/\")[-1])\n",
    "\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        page = html.fromstring(response.text)\n",
    "        links = page.xpath('//*[@id=\"content\"]//p/a/@href')\n",
    "        for link in links:\n",
    "            if \"archives\" in link:\n",
    "                subcollection_name = \"archives\"\n",
    "                info = {\n",
    "                    \"collection_name\": collection_name,\n",
    "                    \"subcollection_name\": subcollection_name,\n",
    "                    \"url\": link,\n",
    "                }\n",
    "                target_urls.append(info)\n",
    "            # I've found that there are some links that cannot be accessed directly. So, I need to get the links from the subcollection page.\n",
    "            else:\n",
    "                response = requests.get(link)\n",
    "                page = html.fromstring(response.text)\n",
    "                links = page.xpath('//*[@id=\"content\"]//p/a/@href')\n",
    "                subcollection_name = urllib.parse.unquote(link.split(\"/\")[-1])\n",
    "                for link in links:\n",
    "                    if \"archives\" in link:\n",
    "                        info = {\n",
    "                            \"collection_name\": collection_name,\n",
    "                            \"subcollection_name\": subcollection_name,\n",
    "                            \"url\": link,\n",
    "                        }\n",
    "                        target_urls.append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in target_urls:\n",
    "    output_path = f\"data/html/{url['collection_name']}/{url['subcollection_name']}\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    response = requests.get(url[\"url\"])\n",
    "    if response.status_code == 200:\n",
    "        page_content = response.text\n",
    "        with open(f\"{output_path}/{url['url'].split('/')[-1]}.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(page_content)\n",
    "        print(f\"Saved {url['url']} to {output_path}/{url['url'].split('/')[-1]}.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import requests\n",
    "from lxml import html\n",
    "import pandas as pd\n",
    "\n",
    "files = glob.glob(\"data/html/*/*/*.html\")\n",
    "data = []\n",
    "\n",
    "for file in files:\n",
    "    try:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file, 'r', encoding='gbk') as f:\n",
    "            content = f.read()\n",
    "\n",
    "    collection_name = file.split(\"\\\\\")[-3]\n",
    "    subcollection_name = file.split(\"\\\\\")[-2]\n",
    "    stamp_page = html.fromstring(content)\n",
    "\n",
    "    page_url = stamp_page.xpath('//*[@class=\"posttitle\"]//a/@href')[0]\n",
    "    stamp_title = stamp_page.xpath('//*[@class=\"posttitle\"]//text()')[0]\n",
    "    stamp_description = \" \".join(stamp_page.xpath('//*[@class=\"postentry\"]/p/text()'))\n",
    "\n",
    "    # 获取大图片的 URL\n",
    "    stamp_images = stamp_page.xpath('//*[@class=\"postentry\"]/p/a/@href')\n",
    "\n",
    "    for stamp_image in stamp_images:\n",
    "        # 生成图片保存路径\n",
    "        image_filename = stamp_image.split(\"/\")[-1]\n",
    "        image_folder = f\"data/images/{collection_name}/{subcollection_name}\"\n",
    "        image_path = os.path.join(image_folder, image_filename)\n",
    "\n",
    "        # 确保目录存在\n",
    "        os.makedirs(image_folder, exist_ok=True)\n",
    "\n",
    "        # 下载并保存图片\n",
    "        try:\n",
    "            response = requests.get(stamp_image, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                with open(image_path, 'wb') as img_file:\n",
    "                    for chunk in response.iter_content(1024):\n",
    "                        img_file.write(chunk)\n",
    "            else:\n",
    "                print(f\"Failed to download {stamp_image}: {response.status_code}\")\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error downloading {stamp_image}: {e}\")\n",
    "\n",
    "        # 记录数据\n",
    "        stamp_info = {\n",
    "            \"url\": page_url,\n",
    "            \"collection_name\": collection_name,\n",
    "            \"subcollection_name\": subcollection_name,\n",
    "            \"title\": stamp_title,\n",
    "            \"image_url\": stamp_image,  # 这里是大图片的 URL\n",
    "            \"image_path\": image_path,\n",
    "            \"description\": stamp_description,\n",
    "        }\n",
    "        data.append(stamp_info)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"stamp_data.csv\", index=False)  # 保存数据到 CSV 文件\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
